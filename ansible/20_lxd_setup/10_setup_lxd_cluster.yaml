---
# 10_setup_lxd_cluster.yaml - Set up LXD cluster across baremetal hosts
# 
# Purpose:
#   Creates and configures an LXD cluster across all baremetal hosts
#   Uses a token-based approach for joining cluster nodes
#   Installs and configures LXD only (no verification)
#
# Requirements:
#   - SSH connectivity between hosts must be configured
#   - Elevated permissions are required
#   - System should be clean (run 19_rollback_lxd_cluster.yaml first)
#
# Run with: 
#   ./scripts/run_ansible.sh ansible/20_lxd_setup/10_setup_lxd_cluster.yaml
#
# Features:
#   - Sets up a ZFS-based storage pool
#   - Creates standard VM profiles
#   - Sets up primary node and joins secondary nodes
#   - Configures network on all members
#   - Does not perform validation (use 18_test_lxd_cluster.yaml for that)

# PLAY 1: Set up primary node
- name: Set up primary node for LXD cluster
  hosts: lxd_primary
  gather_facts: true
  become: true
  
  vars:
    cluster_name: "thinkube-lxd-cluster"
    cluster_port: 8443
    lxd_trust_password: "{{ lookup('env', 'LXD_TRUST_PASSWORD') | default('thinkube-lxd-cluster', true) }}"
    temp_dir: "/tmp/lxd-setup-{{ inventory_hostname }}"
  
  tasks:
    - name: Display primary node information
      ansible.builtin.debug:
        msg: >-
          
          ═════════════════════════════════════════════════════════
          🔧 Setting up LXD Cluster PRIMARY NODE ({{ inventory_hostname }})
          ═════════════════════════════════════════════════════════
          Primary: {{ inventory_hostname }} ({{ ansible_host }})
          Secondary: {{ groups['baremetal'][1:] | join(', ') }}
          
    # Install LXD and ZFS
    - name: Install LXD snap
      community.general.snap:
        name: lxd
        state: present
    
    - name: Install ZFS tools
      ansible.builtin.apt:
        name: zfsutils-linux
        state: present
        update_cache: yes
        
    # Ensure user is added to lxd group
    - name: Add current user to lxd group
      ansible.builtin.user:
        name: "{{ system_username }}"
        groups: lxd
        append: yes
      
    # Ensure proper socket permissions
    - name: Ensure LXD socket has proper permissions
      ansible.builtin.file:
        path: /var/snap/lxd/common/lxd/unix.socket
        mode: '0666'
      failed_when: false  # Socket may not exist yet before initialization
    
    # Create temporary directory
    - name: Create temporary directory for setup files
      ansible.builtin.file:
        path: "{{ temp_dir }}"
        state: directory
        mode: '0755'
    
    # Create preseed file for primary node
    - name: Create preseed file for primary node
      ansible.builtin.copy:
        dest: "{{ temp_dir }}/primary-init.yaml"
        mode: '0644'
        content: |
          config:
            core.https_address: {{ ansible_host }}:{{ cluster_port }}
            core.trust_password: {{ lxd_trust_password }}
          cluster:
            enabled: true
            server_name: {{ inventory_hostname }}
          storage_pools:
          - name: default
            driver: zfs
            config:
              size: 800GB
          networks:
          - name: {{ lxd_network_name }}
            type: bridge
            config:
              ipv4.address: {{ lxd_network_ipv4_address }}
              ipv4.nat: "true"
              ipv6.address: {{ lxd_network_ipv6_address }}
              ipv6.nat: "true"
              dns.mode: managed
          profiles:
          - name: default
            devices:
              eth0:
                name: eth0
                network: {{ lxd_network_name }}
                type: nic
              root:
                path: /
                pool: default
                type: disk
    
    # Check if LXD is already initialized
    - name: Check if LXD is already initialized
      ansible.builtin.command: lxc list
      register: lxd_initialized
      failed_when: false
      changed_when: false

    # Check if LXD is in cluster mode
    - name: Check if LXD is in cluster mode
      ansible.builtin.command: lxc cluster list
      register: lxd_cluster_check
      failed_when: false
      changed_when: false
      when: lxd_initialized.rc == 0

    # Initialize LXD on primary node if not initialized OR not in cluster mode
    - name: Initialize LXD on primary node with preseed
      ansible.builtin.shell: |
        cat {{ temp_dir }}/primary-init.yaml | lxd init --preseed
      register: primary_init
      when: lxd_initialized.rc != 0 or (lxd_initialized.rc == 0 and lxd_cluster_check.rc != 0)
    
    # Wait for initialization to complete
    - name: Wait for LXD initialization on primary node
      ansible.builtin.pause:
        seconds: 10
        prompt: "Waiting for LXD to initialize on primary node..."
      when: lxd_initialized.rc != 0 or (lxd_initialized.rc == 0 and lxd_cluster_check.rc != 0)

    # Verify LXD initialization
    - name: Verify LXD is initialized and running
      ansible.builtin.command: lxc list
      register: lxc_ready
      retries: 10
      delay: 3
      until: lxc_ready.rc == 0
    
    # Create VM profiles (optional but useful)
    - name: Create VM profile templates
      block:
        # Create vm-networks profile
        - name: Create vm-networks profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-networks.yaml"
            mode: '0644'
            content: |
              name: vm-networks
              description: Profile for VM networking setup
              config:
                security.nesting: "true"
              devices:
                eth0:
                  name: eth0
                  nictype: bridged
                  parent: {{ lxd_network_name }}
                  type: nic
                eth1:
                  name: eth1
                  nictype: bridged
                  parent: br0
                  type: nic
        
        - name: Apply vm-networks profile
          ansible.builtin.shell: |
            lxc profile create vm-networks 2>/dev/null || true
            cat {{ temp_dir }}/vm-networks.yaml | lxc profile edit vm-networks
        
        # Create vm-resources profile
        - name: Create vm-resources profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-resources.yaml"
            mode: '0644'
            content: |
              name: vm-resources
              description: Profile for VM resource limits
              config:
                limits.cpu: "4"
                limits.memory: 4GB
                limits.memory.enforce: hard
                security.secureboot: "false"
                boot.autostart: "true"
              devices: {}
        
        - name: Apply vm-resources profile
          ansible.builtin.shell: |
            lxc profile create vm-resources 2>/dev/null || true
            cat {{ temp_dir }}/vm-resources.yaml | lxc profile edit vm-resources
        
        # Create vm-gpu profile
        - name: Create vm-gpu profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-gpu.yaml"
            mode: '0644'
            content: |
              name: vm-gpu
              description: Profile for VM GPU passthrough
              config:
                nvidia.driver.capabilities: all
                nvidia.runtime: "true"
              devices: {}
        
        - name: Apply vm-gpu profile
          ansible.builtin.shell: |
            lxc profile create vm-gpu 2>/dev/null || true
            cat {{ temp_dir }}/vm-gpu.yaml | lxc profile edit vm-gpu
    
    # Generate and store tokens for all secondary nodes
    - name: Generate join tokens for secondary nodes
      ansible.builtin.command: |
        lxc cluster add {{ item }}
      register: token_results
      loop: "{{ groups['lxd_secondary'] }}"
      failed_when: false  # Don't fail if node already exists in cluster
    
    - name: Extract and save tokens for each node
      ansible.builtin.set_fact:
        cluster_tokens: "{{ cluster_tokens | default({}) | combine({item.item: item.stdout_lines[1] | default('')}) }}"
        cacheable: true
      loop: "{{ token_results.results }}"
      when: item.rc == 0
    
    - name: Save tokens to files for secondary nodes
      ansible.builtin.copy:
        content: "{{ cluster_tokens[item] }}"
        dest: "/tmp/lxd_cluster_token_{{ item }}.txt"
        mode: '0644'
      loop: "{{ groups['lxd_secondary'] }}"
      when: cluster_tokens[item] is defined
    
    - name: Display token information (truncated)
      ansible.builtin.debug:
        msg: "Generated tokens for secondary nodes: {{ cluster_tokens.keys() | list }}"
      when: cluster_tokens is defined and cluster_tokens | length > 0
    
    - name: Display single node cluster message
      ansible.builtin.debug:
        msg: "No secondary nodes found - LXD cluster will run on single node"
      when: groups['lxd_secondary'] | default([]) | length == 0

# PLAY 2: Join secondary nodes to the cluster
- name: Join secondary nodes to LXD cluster
  hosts: lxd_secondary
  gather_facts: true
  become: true
  
  vars:
    primary_node: "{{ groups['lxd_primary'][0] }}"
    temp_dir: "/tmp/lxd-setup-{{ inventory_hostname }}"
    lxd_trust_password: "{{ lookup('env', 'LXD_TRUST_PASSWORD') | default('thinkube-lxd-cluster', true) }}"
  
  tasks:
    - name: Display secondary node information
      ansible.builtin.debug:
        msg: >-
          
          ═════════════════════════════════════════════════════════
          🔄 Joining LXD Cluster as SECONDARY NODE ({{ inventory_hostname }})
          ═════════════════════════════════════════════════════════
          Primary: {{ primary_node }} ({{ hostvars[primary_node]['ansible_host'] }})
          This node: {{ inventory_hostname }} ({{ ansible_host }})
          
    # Install required software
    - name: Install LXD snap
      community.general.snap:
        name: lxd
        state: present
    
    - name: Install ZFS tools
      ansible.builtin.apt:
        name: zfsutils-linux
        state: present
        update_cache: yes
        
    # Ensure user is added to lxd group
    - name: Add current user to lxd group
      ansible.builtin.user:
        name: "{{ system_username }}"
        groups: lxd
        append: yes
      
    # Ensure proper socket permissions
    - name: Ensure LXD socket has proper permissions
      ansible.builtin.file:
        path: /var/snap/lxd/common/lxd/unix.socket
        mode: '0666'
      failed_when: false  # Socket may not exist yet before initialization
    
    # Create temporary directory
    - name: Create temporary directory for setup files
      ansible.builtin.file:
        path: "{{ temp_dir }}"
        state: directory
        mode: '0755'
    
    # Get join token from primary node (specific to this node)
    - name: Get cluster join token from primary node
      ansible.builtin.slurp:
        src: "/tmp/lxd_cluster_token_{{ inventory_hostname }}.txt"
      register: token_file_b64
      delegate_to: "{{ primary_node }}"
    
    - name: Extract token from file
      ansible.builtin.set_fact:
        join_token: "{{ token_file_b64['content'] | b64decode | trim }}"
    
    - name: Display token information (truncated)
      ansible.builtin.debug:
        msg: "Using token from primary node: {{ join_token | truncate(20, true) }}..."
    
    # Create preseed file for joining
    - name: Create preseed file for joining the cluster
      ansible.builtin.copy:
        dest: "{{ temp_dir }}/join-preseed.yaml"
        mode: '0644'
        content: |
          cluster:
            enabled: true
            server_name: {{ inventory_hostname }}
            server_address: {{ ansible_host }}:8443
            cluster_address: {{ hostvars[primary_node]['ansible_host'] }}:8443
            cluster_token: {{ join_token }}
            # Member-specific configuration must only include keys that can be different across nodes
            # For a pure ZFS/dir storage pool, we only need an empty source
            member_config:
            - entity: storage-pool
              name: default
              key: source
              value: ""
          config:
            core.https_address: {{ ansible_host }}:8443
            core.trust_password: {{ lxd_trust_password }}
    
    # Check for existing network bridge and remove it if needed
    - name: Check for existing network bridge and remove if present
      ansible.builtin.shell: |
        if ip link show {{ lxd_network_name }} 2>/dev/null; then
          echo "Removing existing {{ lxd_network_name }} device..."
          ip link set {{ lxd_network_name }} down || true
          brctl delbr {{ lxd_network_name }} || true
        else
          echo "No existing {{ lxd_network_name }} found"
        fi
      register: bridge_cleanup
      changed_when: "'Removing existing' in bridge_cleanup.stdout"

    # Check if node is already in cluster
    - name: Check if node is already in cluster
      ansible.builtin.shell: |
        lxc cluster list 2>/dev/null | grep -q "{{ inventory_hostname }}" && echo "already_joined" || echo "not_joined"
      register: cluster_join_status
      failed_when: false
      changed_when: false
      
    # Join the cluster only if not already joined
    - name: Join the cluster using preseed file
      ansible.builtin.shell: |
        cat {{ temp_dir }}/join-preseed.yaml | lxd init --preseed --debug
      register: join_result
      failed_when: false
      when: cluster_join_status.stdout == "not_joined"
      async: 180  # Allow 3 minutes for the join operation
      poll: 10    # Check every 10 seconds

    # Output detailed error information if join fails
    - name: Display join result
      ansible.builtin.debug:
        msg: |
          {% if cluster_join_status.stdout == "already_joined" %}
          Join result: Node already joined to cluster
          {% else %}
          Join result: {{ join_result.rc | default(0) == 0 | ternary('SUCCESS', 'FAILED') }}
          Return code: {{ join_result.rc | default(0) }}

          {{ join_result.stderr | default('No error message') }}
          {% endif %}
    
    # Wait for join to complete (only if we just joined)
    - name: Wait for cluster join to complete
      ansible.builtin.pause:
        seconds: 20
        prompt: "Waiting for secondary node to join cluster..."
      when: cluster_join_status.stdout == "not_joined"

    # Verify join completed with retries and clear output
    - name: Verify join completed
      ansible.builtin.shell: |
        for i in $(seq 1 10); do
          if lxc cluster list 2>/dev/null | grep -q "{{ inventory_hostname }}"; then
            echo "Joined successfully"
            exit 0
          fi
          echo "Waiting for join to complete... Attempt $i/10"
          sleep 5
        done
        echo "Failed to verify cluster join"
        exit 1
      register: join_complete
      changed_when: "join_complete.stdout is search('Joined successfully')"
      retries: 3
      delay: 10
      until: join_complete.rc == 0

# PLAY 3: Force network initialization on all nodes
- name: Force network initialization on all nodes
  hosts: "{{ groups['baremetal'][0] }}"  # Primary node (bcn2)
  gather_facts: true
  become: true

  tasks:
    - name: Pause to ensure cluster is stable
      ansible.builtin.pause:
        seconds: 10
        prompt: "Pausing briefly before network synchronization..."

    - name: Check if cluster is properly formed
      ansible.builtin.shell: |
        lxc cluster list | grep -c "ONLINE" || echo "0"
      register: cluster_check
      failed_when: false
      changed_when: false

    - name: Display cluster members found
      ansible.builtin.debug:
        msg: |
          Current cluster status:
          {{ cluster_check.stdout }}

    # Check if network already exists
    - name: Check if network already exists
      ansible.builtin.shell: |
        lxc network list | grep -q "{{ lxd_network_name }}" && echo "exists" || echo "not_exists"
      register: network_exists
      changed_when: false
      
    # Recreate network on all cluster members
    - name: Recreate network on all cluster members
      ansible.builtin.shell: |
        # Delete existing network if it exists
        if [[ "{{ network_exists.stdout }}" == "exists" ]]; then
          echo "Deleting existing network {{ lxd_network_name }}..."
          lxc network delete {{ lxd_network_name }} --force 2>/dev/null || true
          # Give it a moment to propagate the delete
          sleep 5
        fi
        
        # Check if we have secondary nodes and they are online
        {% if groups['baremetal'] | length > 1 %}
        if lxc cluster list 2>/dev/null | grep -q "{{ groups['baremetal'][1] }}.*ONLINE"; then
          echo "Cluster appears to be fully formed, synchronizing network to all members"
        {% else %}
        if true; then
          echo "Single node cluster, creating network..."
        {% endif %}
          
          # Create network with target=all
          echo "Creating network with target=all parameter..."
          lxc network create {{ lxd_network_name }} \
            ipv4.address={{ lxd_network_ipv4_address }} \
            ipv4.nat=true \
            ipv6.address={{ lxd_network_ipv6_address }} \
            ipv6.nat=true \
            dns.mode=managed \
            --target=all

          # Check that network is created successfully with timeout
          for i in {1..10}; do
            if lxc network show {{ lxd_network_name }} &>/dev/null; then
              echo "Network created successfully"
              lxc network show {{ lxd_network_name }}
              break
            fi
            echo "Waiting for network to be ready... (attempt $i/10)"
            sleep 5
          done
        else
          echo "WARNING: Secondary node not detected as ONLINE in cluster!"
          echo "Creating network on primary node only..."

          # Create network on primary only
          lxc network create {{ lxd_network_name }} \
            ipv4.address={{ lxd_network_ipv4_address }} \
            ipv4.nat=true \
            ipv6.address={{ lxd_network_ipv6_address }} \
            ipv6.nat=true \
            dns.mode=managed
            
          echo "Network created on primary node only"
          lxc network show {{ lxd_network_name }}
        fi
      register: network_sync_result
      changed_when: true
      async: 180  # Allow 3 minutes for network creation
      poll: 10    # Check every 10 seconds

    - name: Display network synchronization result
      ansible.builtin.debug:
        msg: |
          Network synchronization result:
          {{ network_sync_result.stdout }}
    
    # Wait for network creation to propagate
    - name: Wait for networks to propagate to all cluster members
      ansible.builtin.pause:
        seconds: 15
        prompt: "Waiting for network configuration to propagate to all cluster members..."
    
    # Restart LXD on all nodes to ensure network interfaces are created
    - name: Check LXD daemon status
      ansible.builtin.systemd:
        name: snap.lxd.daemon
      register: lxd_status
      
    - name: Restart LXD service on primary node if needed
      ansible.builtin.systemd:
        name: snap.lxd.daemon
        state: restarted
        daemon_reload: yes
      register: lxd_restart
    
    - name: Wait for LXD service to fully restart on primary
      ansible.builtin.pause:
        seconds: 15
        prompt: "Waiting for LXD service to fully restart on primary node..."
        
    # Verify LXD is running after restart
    - name: Verify LXD is running
      ansible.builtin.command: lxc list
      register: lxd_verify
      retries: 5
      delay: 5
      until: lxd_verify.rc == 0
    
    # Ensure network interfaces are UP on all nodes
    - name: Ensure network interfaces are UP
      ansible.builtin.shell: |
        # Set interface UP on primary node
        if ip link show {{ lxd_network_name }} &>/dev/null; then
          echo "{{ inventory_hostname }}: Setting {{ lxd_network_name }} UP"
          ip link set {{ lxd_network_name }} up promisc on
          echo "{{ inventory_hostname }}: Interface is now $(ip link show {{ lxd_network_name }} | grep -o 'state [A-Z]*')"
        else
          echo "{{ inventory_hostname }}: ✗ {{ lxd_network_name }} interface MISSING"
        fi
      register: primary_interface
      changed_when: "'Setting' in primary_interface.stdout"
      
    - name: Display primary interface status
      ansible.builtin.debug:
        msg: |
          Primary node interface status:
          {{ primary_interface.stdout_lines | join('\n') }}

    - name: Final status message
      ansible.builtin.debug:
        msg: >-

          ═════════════════════════════════════════════════════════
          ✅ LXD CLUSTER SETUP COMPLETE ON PRIMARY NODE
          ═════════════════════════════════════════════════════════

          LXD cluster has been set up on the primary node.
          Secondary nodes are now being configured.

          PROFILES CREATED:
          - default: Basic container profile
          - vm-networks: Network configuration for VMs
          - vm-resources: Resource limits for VMs
          - vm-gpu: GPU passthrough configuration

# PLAY 4: Configure secondary nodes
- name: Configure secondary nodes
  hosts: "{{ groups['baremetal'][1:] }}"  # Secondary nodes
  gather_facts: true
  become: true

  tasks:
    - name: Display host information
      ansible.builtin.debug:
        msg: >-
          
          ═════════════════════════════════════════════════════════
          🔧 Configuring Network on Secondary Node {{ inventory_hostname }}
          ═════════════════════════════════════════════════════════

    # Check if LXD is running on secondary node
    - name: Check LXD status on secondary node
      ansible.builtin.systemd:
        name: snap.lxd.daemon
      register: secondary_lxd_status
      
    # Restart LXD to ensure network interfaces are created
    - name: Restart LXD service on secondary node
      ansible.builtin.systemd:
        name: snap.lxd.daemon
        state: restarted
        daemon_reload: yes
      register: secondary_lxd_restart
      
    - name: Wait for LXD service to restart
      ansible.builtin.pause:
        seconds: 15
        prompt: "Waiting for LXD service to restart on {{ inventory_hostname }}..."
        
    # Verify LXD is running on secondary
    - name: Verify LXD is running on secondary
      ansible.builtin.command: lxc list
      register: secondary_lxd_verify
      retries: 5
      delay: 5
      until: secondary_lxd_verify.rc == 0
    
    # Ensure network interfaces are UP
    - name: Ensure network interfaces are UP
      ansible.builtin.shell: |
        # Check if interface exists
        if ip link show {{ lxd_network_name }} &>/dev/null; then
          echo "Setting {{ lxd_network_name }} UP"
          ip link set {{ lxd_network_name }} up promisc on
          echo "Interface is now $(ip link show {{ lxd_network_name }} | grep -o 'state [A-Z]*')"
        else
          echo "✗ {{ lxd_network_name }} interface MISSING"
        fi
      register: interface_status
      changed_when: "'Setting' in interface_status.stdout"
      
    - name: Display interface status
      ansible.builtin.debug:
        msg: |
          Interface status on {{ inventory_hostname }}:
          {{ interface_status.stdout_lines | join('\n') }}

# PLAY 5: Final status check
- name: Final status check
  hosts: "{{ groups['baremetal'][0] }}"  # Primary node
  gather_facts: true
  become: true

  tasks:
    - name: Check cluster status
      ansible.builtin.command: |
        lxc cluster list
      register: final_cluster_status
      changed_when: false
      
    - name: Display final cluster status
      ansible.builtin.debug:
        msg: |
          Final cluster status:
          {{ final_cluster_status.stdout }}
          
    - name: Final completion message
      ansible.builtin.debug:
        msg: >-

          ═════════════════════════════════════════════════════════
          ✅ LXD CLUSTER SETUP COMPLETE
          ═════════════════════════════════════════════════════════

          LXD cluster has been set up across all nodes.

          PROFILES CREATED:
          - default: Basic container profile
          - vm-networks: Network configuration for VMs
          - vm-resources: Resource limits for VMs
          - vm-gpu: GPU passthrough configuration

          NETWORK INTERFACES:
          - lxdbr0 interfaces are created on all nodes
          - Note: These interfaces may show state "DOWN" - this is normal when no containers are using them
          - They will automatically transition to UP state once containers/VMs are started

          NEXT STEPS:
          Run 18_test_lxd_cluster.yaml to validate the setup