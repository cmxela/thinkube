---
# 10_setup_lxd_cluster.yaml - Set up LXD cluster across baremetal hosts
# 
# Purpose:
#   Creates and configures an LXD cluster across all baremetal hosts
#   Uses a token-based approach for joining cluster nodes
#   Installs and configures LXD only (no verification)
#
# Requirements:
#   - SSH connectivity between hosts must be configured
#   - Elevated permissions are required
#   - System should be clean (run 19_rollback_lxd_cluster.yaml first)
#
# Run with: 
#   ./scripts/run_ansible.sh ansible/20_lxd_setup/10_setup_lxd_cluster.yaml
#
# Features:
#   - Sets up a ZFS-based storage pool
#   - Creates standard VM profiles
#   - Sets up primary node and joins secondary nodes
#   - Configures network on all members
#   - Does not perform validation (use 18_test_lxd_cluster.yaml for that)

# PLAY 1: Set up primary node
- name: Set up primary node for LXD cluster
  hosts: "{{ groups['baremetal'][0] }}"  # Primary node (bcn2)
  gather_facts: true
  become: true
  
  vars:
    cluster_name: "thinkube-lxd-cluster"
    cluster_port: 8443
    lxd_trust_password: "{{ lookup('env', 'LXD_TRUST_PASSWORD') | default('thinkube-lxd-cluster', true) }}"
    temp_dir: "/tmp/lxd-setup-{{ inventory_hostname }}"
  
  tasks:
    - name: Display primary node information
      ansible.builtin.debug:
        msg: >-
          
          ═════════════════════════════════════════════════════════
          🔧 Setting up LXD Cluster PRIMARY NODE ({{ inventory_hostname }})
          ═════════════════════════════════════════════════════════
          Primary: {{ inventory_hostname }} ({{ ansible_host }})
          Secondary: {{ groups['baremetal'][1:] | join(', ') }}
          
    # Install LXD and ZFS
    - name: Install LXD snap
      community.general.snap:
        name: lxd
        state: present
    
    - name: Install ZFS tools
      ansible.builtin.apt:
        name: zfsutils-linux
        state: present
        update_cache: yes
    
    # Create temporary directory
    - name: Create temporary directory for setup files
      ansible.builtin.file:
        path: "{{ temp_dir }}"
        state: directory
        mode: '0755'
    
    # Create preseed file for primary node
    - name: Create preseed file for primary node
      ansible.builtin.copy:
        dest: "{{ temp_dir }}/primary-init.yaml"
        mode: '0644'
        content: |
          config:
            core.https_address: {{ ansible_host }}:{{ cluster_port }}
            core.trust_password: {{ lxd_trust_password }}
          cluster:
            enabled: true
            server_name: {{ inventory_hostname }}
          storage_pools:
          - name: default
            driver: zfs
          networks:
          - name: {{ lxd_network_name }}
            type: bridge
            config:
              ipv4.address: {{ lxd_network_ipv4_address }}
              ipv4.nat: "true"
              ipv6.address: {{ lxd_network_ipv6_address }}
              ipv6.nat: "true"
              dns.mode: managed
          profiles:
          - name: default
            devices:
              eth0:
                name: eth0
                network: {{ lxd_network_name }}
                type: nic
              root:
                path: /
                pool: default
                type: disk
    
    # Initialize LXD on primary node
    - name: Initialize LXD on primary node with preseed
      ansible.builtin.shell: |
        cat {{ temp_dir }}/primary-init.yaml | lxd init --preseed
      register: primary_init
    
    # Wait for initialization to complete
    - name: Wait for LXD initialization on primary node
      ansible.builtin.pause:
        seconds: 5
        prompt: "Waiting for LXD to initialize on primary node..."

    # Verify LXD initialization
    - name: Verify LXD is initialized and running
      ansible.builtin.command: lxc list
      register: lxc_ready
      retries: 5
      delay: 2
      until: lxc_ready.rc == 0
    
    # Create VM profiles (optional but useful)
    - name: Create VM profile templates
      block:
        # Create vm-networks profile
        - name: Create vm-networks profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-networks.yaml"
            mode: '0644'
            content: |
              name: vm-networks
              description: Profile for VM networking setup
              config:
                security.nesting: "true"
              devices:
                eth0:
                  name: eth0
                  nictype: bridged
                  parent: {{ lxd_network_name }}
                  type: nic
                eth1:
                  name: eth1
                  nictype: bridged
                  parent: br0
                  type: nic
        
        - name: Apply vm-networks profile
          ansible.builtin.shell: |
            lxc profile create vm-networks 2>/dev/null || true
            cat {{ temp_dir }}/vm-networks.yaml | lxc profile edit vm-networks
        
        # Create vm-resources profile
        - name: Create vm-resources profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-resources.yaml"
            mode: '0644'
            content: |
              name: vm-resources
              description: Profile for VM resource limits
              config:
                limits.cpu: "4"
                limits.memory: 4GB
                limits.memory.enforce: hard
                security.secureboot: "false"
                boot.autostart: "true"
              devices: {}
        
        - name: Apply vm-resources profile
          ansible.builtin.shell: |
            lxc profile create vm-resources 2>/dev/null || true
            cat {{ temp_dir }}/vm-resources.yaml | lxc profile edit vm-resources
        
        # Create vm-gpu profile
        - name: Create vm-gpu profile
          ansible.builtin.copy:
            dest: "{{ temp_dir }}/vm-gpu.yaml"
            mode: '0644'
            content: |
              name: vm-gpu
              description: Profile for VM GPU passthrough
              config:
                nvidia.driver.capabilities: all
                nvidia.runtime: "true"
              devices: {}
        
        - name: Apply vm-gpu profile
          ansible.builtin.shell: |
            lxc profile create vm-gpu 2>/dev/null || true
            cat {{ temp_dir }}/vm-gpu.yaml | lxc profile edit vm-gpu
    
    # Generate and store token for secondary nodes
    - name: Generate join token for bcn1
      ansible.builtin.command: |
        lxc cluster add bcn1
      register: token_result
    
    - name: Extract and save token
      ansible.builtin.set_fact:
        cluster_token: "{{ token_result.stdout_lines[1] | default('') }}"
        cacheable: true
    
    - name: Save token to file for secondary nodes
      ansible.builtin.copy:
        content: "{{ cluster_token }}"
        dest: "/tmp/lxd_cluster_token.txt"
        mode: '0644'
    
    - name: Display token information (truncated)
      ansible.builtin.debug:
        msg: "Generated token for secondary nodes: {{ cluster_token | truncate(20, true) }}..."

# PLAY 2: Join secondary nodes to the cluster
- name: Join secondary nodes to LXD cluster
  hosts: "{{ groups['baremetal'][1:] }}"  # Secondary nodes (bcn1, etc.)
  gather_facts: true
  become: true
  
  vars:
    primary_node: "{{ groups['baremetal'][0] }}"  # First node (bcn2)
    temp_dir: "/tmp/lxd-setup-{{ inventory_hostname }}"
    lxd_trust_password: "{{ lookup('env', 'LXD_TRUST_PASSWORD') | default('thinkube-lxd-cluster', true) }}"
  
  tasks:
    - name: Display secondary node information
      ansible.builtin.debug:
        msg: >-
          
          ═════════════════════════════════════════════════════════
          🔄 Joining LXD Cluster as SECONDARY NODE ({{ inventory_hostname }})
          ═════════════════════════════════════════════════════════
          Primary: {{ primary_node }} ({{ hostvars[primary_node]['ansible_host'] }})
          This node: {{ inventory_hostname }} ({{ ansible_host }})
          
    # Install required software
    - name: Install LXD snap
      community.general.snap:
        name: lxd
        state: present
    
    - name: Install ZFS tools
      ansible.builtin.apt:
        name: zfsutils-linux
        state: present
        update_cache: yes
    
    # Create temporary directory
    - name: Create temporary directory for setup files
      ansible.builtin.file:
        path: "{{ temp_dir }}"
        state: directory
        mode: '0755'
    
    # Get join token from primary node
    - name: Get cluster join token from primary node
      ansible.builtin.slurp:
        src: "/tmp/lxd_cluster_token.txt"
      register: token_file_b64
      delegate_to: "{{ primary_node }}"
    
    - name: Extract token from file
      ansible.builtin.set_fact:
        join_token: "{{ token_file_b64['content'] | b64decode | trim }}"
    
    - name: Display token information (truncated)
      ansible.builtin.debug:
        msg: "Using token from primary node: {{ join_token | truncate(20, true) }}..."
    
    # Create preseed file for joining
    - name: Create preseed file for joining the cluster
      ansible.builtin.copy:
        dest: "{{ temp_dir }}/join-preseed.yaml"
        mode: '0644'
        content: |
          cluster:
            enabled: true
            server_name: {{ inventory_hostname }}
            server_address: {{ ansible_host }}:8443
            cluster_address: {{ hostvars[primary_node]['ansible_host'] }}:8443
            cluster_token: {{ join_token }}
            # Member-specific configuration must only include keys that can be different across nodes
            # For a pure ZFS/dir storage pool, we only need an empty source
            member_config:
            - entity: storage-pool
              name: default
              key: source
              value: ""
          config:
            core.https_address: {{ ansible_host }}:8443
            core.trust_password: {{ lxd_trust_password }}
    
    # Check for existing network bridge and remove it if needed
    - name: Check for existing network bridge and remove if present
      ansible.builtin.shell: |
        if ip link show {{ lxd_network_name }} 2>/dev/null; then
          echo "Removing existing {{ lxd_network_name }} device..."
          ip link set {{ lxd_network_name }} down || true
          brctl delbr {{ lxd_network_name }} || true
        else
          echo "No existing {{ lxd_network_name }} found"
        fi
      register: bridge_cleanup
      changed_when: "'Removing existing' in bridge_cleanup.stdout"

    # Join the cluster
    - name: Join the cluster using preseed file
      ansible.builtin.shell: |
        cat {{ temp_dir }}/join-preseed.yaml | lxd init --preseed --debug
      register: join_result
      failed_when: false

    # Output detailed error information if join fails
    - name: Display join result
      ansible.builtin.debug:
        msg: |
          Join result: {{ join_result.rc == 0 | ternary('SUCCESS', 'FAILED') }}
          Return code: {{ join_result.rc }}

          {{ join_result.stderr | default('No error message') }}
    
    # Wait for join to complete
    - name: Wait for cluster join to complete
      ansible.builtin.pause:
        seconds: 10
        prompt: "Waiting for secondary node to join cluster..."

    # Verify join completed
    - name: Verify join completed
      ansible.builtin.command: lxc cluster list
      register: join_complete
      retries: 3
      delay: 5
      until: join_complete.rc == 0 and inventory_hostname in join_complete.stdout

# PLAY 3: Force network initialization on all nodes
- name: Force network initialization on all nodes
  hosts: "{{ groups['baremetal'][0] }}"  # Primary node (bcn2)
  gather_facts: true
  become: true

  tasks:
    - name: Pause to ensure cluster is stable
      ansible.builtin.pause:
        seconds: 10
        prompt: "Pausing briefly before network synchronization..."

    - name: Check if cluster is properly formed
      ansible.builtin.command: |
        lxc cluster list
      register: cluster_check
      failed_when: false

    - name: Display cluster members found
      ansible.builtin.debug:
        msg: |
          Current cluster status:
          {{ cluster_check.stdout }}

    - name: Recreate network on all cluster members
      ansible.builtin.shell: |
        if lxc cluster list 2>/dev/null | grep -q "{{ groups['baremetal'][1] }}"; then
          echo "Cluster appears to be formed, synchronizing network to all members"

          # Delete existing network first if it exists
          lxc network delete {{ lxd_network_name }} --force 2>/dev/null || true
          sleep 5

          # Recreate network with target=all to ensure it spreads across all nodes
          echo "Creating network with target=all parameter..."
          lxc network create {{ lxd_network_name }} \
            ipv4.address={{ lxd_network_ipv4_address }} \
            ipv4.nat=true \
            ipv6.address={{ lxd_network_ipv6_address }} \
            ipv6.nat=true \
            --target=all

          # Check that network is created successfully
          for i in {1..5}; do
            if lxc network show {{ lxd_network_name }} &>/dev/null; then
              echo "Network created successfully"
              break
            fi
            echo "Waiting for network to be ready... (attempt $i/5)"
            sleep 3
          done
        else
          echo "WARNING: Cluster does not appear to be properly formed!"
          echo "Attempting to create network anyway..."

          # Try recreating network on primary
          lxc network delete {{ lxd_network_name }} --force 2>/dev/null || true
          sleep 5
          lxc network create {{ lxd_network_name }} \
            ipv4.address={{ lxd_network_ipv4_address }} \
            ipv4.nat=true \
            ipv6.address={{ lxd_network_ipv6_address }} \
            ipv6.nat=true
        fi
      register: network_sync_result

    - name: Display network synchronization result
      ansible.builtin.debug:
        msg: |
          Network synchronization result:
          {{ network_sync_result.stdout }}
    
    # Ensure network interfaces are UP on all nodes
    - name: Ensure all lxdbr0 interfaces are UP
      ansible.builtin.shell: |
        cd {{ playbook_dir }}/../../
        # First check the interface state on each node
        for node in $(lxc cluster list | grep -o "bcn[0-9]"); do
          echo "Checking interface state on $node..."
          STATE=$(ssh -o BatchMode=yes -o ConnectTimeout=5 $node "ip link show {{ lxd_network_name }} | grep -o 'state [A-Z]*'" 2>/dev/null || echo "NOT FOUND")
          if [[ "$STATE" == "state DOWN" ]]; then
            echo "Interface is DOWN on $node, setting to UP state"
            ./scripts/run_ssh_command.sh $node "ip link set {{ lxd_network_name }} up promisc on"
            sleep 2
            NEW_STATE=$(ssh -o BatchMode=yes -o ConnectTimeout=5 $node "ip link show {{ lxd_network_name }} | grep -o 'state [A-Z]*'" 2>/dev/null || echo "NOT FOUND")
            echo "New state on $node: $NEW_STATE"
          elif [[ "$STATE" == "NOT FOUND" ]]; then
            echo "ERROR: Interface {{ lxd_network_name }} not found on $node!"
          else
            echo "Interface on $node is already $STATE"
          fi
        done
      register: interface_fixes
      changed_when: "'setting to UP state' in interface_fixes.stdout"

    - name: Display interface status after fixes
      ansible.builtin.debug:
        msg: |
          Interface status check results:
          {{ interface_fixes.stdout_lines | join('\n') }}

    - name: Final status message
      ansible.builtin.debug:
        msg: >-

          ═════════════════════════════════════════════════════════
          ✅ LXD CLUSTER SETUP COMPLETE
          ═════════════════════════════════════════════════════════

          LXD cluster has been set up across all nodes.

          PROFILES CREATED:
          - default: Basic container profile
          - vm-networks: Network configuration for VMs
          - vm-resources: Resource limits for VMs
          - vm-gpu: GPU passthrough configuration

          NETWORK INTERFACES:
          - lxdbr0 interfaces are created on all nodes
          - Note: These interfaces may show state "DOWN" - this is normal when no containers are using them
          - They will automatically transition to UP state once containers/VMs are started

          NEXT STEPS:
          Run 18_test_lxd_cluster.yaml to validate the setup